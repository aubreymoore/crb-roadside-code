{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Example usage:\n",
    "\n",
    "    cd ~/Guam02/code/object-detectors\n",
    "    source ~/Guam02/code/py38env/bin/activate\n",
    "    python3.6 demo.py --video /home/aubreytensor1/Guam02/rawdata/20201211_134207.mp4 --output_dir /home/aubreytensor1/Guam02/output --num_frames 5 --skip_no 1 --dump_sql False\n",
    "\n",
    "Mod by Aubrey Moore 2020-12-25: added else clause:\n",
    "\n",
    "    if width > 1920 or height > 1080:\n",
    "        image = image_org.resize((width // 2, height // 2), Image.ANTIALIAS)\n",
    "    else:\n",
    "        image = image_org\n",
    "\n",
    "Mod by Aubrey Moore 2020-12-25: added --output_dir argument\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# Set environment variables\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# import tensorflow as tf\n",
    "# uncomment following lines if you are using TF2\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import cv2\n",
    "import argparse\n",
    "from PIL import Image\n",
    "# from gpslogger import GPSLogger\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('./Mask_RCNN')\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.model as modellib\n",
    "import skimage.io\n",
    "from collections import OrderedDict\n",
    "from skimage.measure import find_contours, approximate_polygon\n",
    "from xml_dumper import dump_as_cvat_annotation\n",
    "from sql_dumper import dump_to_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetection:\n",
    "    def __init__(self, model_path):\n",
    "        self.detection_graph = tf.Graph()\n",
    "        with self.detection_graph.as_default():\n",
    "            od_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(model_path , 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "                config = tf.ConfigProto()\n",
    "                config.gpu_options.allow_growth=True\n",
    "                self.sess = tf.Session(graph=self.detection_graph, config=config)\n",
    "\n",
    "    def get_detections(self, image_np_expanded):\n",
    "        image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        (boxes, scores, classes, num_detections) = self.sess.run([boxes, scores, classes, num_detections], feed_dict={image_tensor: image_np_expanded})\n",
    "        return boxes, scores, classes, num_detections\n",
    "\n",
    "    @staticmethod\n",
    "    def process_boxes(boxes, scores, classes, labels_mapping, threshold, width, height):\n",
    "        result = {}\n",
    "        for i in range(len(classes[0])):\n",
    "            if classes[0][i] in labels_mapping.keys():\n",
    "                if scores[0][i] >= threshold:\n",
    "                    xmin = int(boxes[0][i][1] * width)\n",
    "                    ymin = int(boxes[0][i][0] * height)\n",
    "                    xmax = int(boxes[0][i][3] * width)\n",
    "                    ymax = int(boxes[0][i][2] * height)\n",
    "                    label = labels_mapping[classes[0][i]]\n",
    "                    if label not in result:\n",
    "                        result[label] = []\n",
    "                    result[label].append([xmin,ymin,xmax,ymax])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation:\n",
    "    def __init__(self, model_path, num_c=2):\n",
    "        class InferenceConfig(Config):\n",
    "            # Set batch size to 1 since we'll be running inference on\n",
    "            # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "            NAME = \"cvat\"\n",
    "            GPU_COUNT = 1\n",
    "            IMAGES_PER_GPU = 1\n",
    "            NUM_CLASSES = num_c\n",
    "\n",
    "        config = InferenceConfig()\n",
    "        #config.display()\n",
    "\n",
    "        # Create model object in inference mode.\n",
    "        self.model = modellib.MaskRCNN(mode=\"inference\", model_dir=\"./output\", config=config)\n",
    "        # Load weights trained on MS-COCO\n",
    "        self.model.load_weights(model_path, by_name=True)\n",
    "        self.labels_mapping = {0:'BG', 1:'cut'}\n",
    "\n",
    "    def get_polygons(self, images, threshold):\n",
    "        res = self.model.detect(images)\n",
    "        result = {}\n",
    "        for r in res:\n",
    "            for index, c_id in enumerate(r['class_ids']):\n",
    "                if c_id in self.labels_mapping.keys():\n",
    "                    if r['scores'][index] >= threshold:\n",
    "                        mask = r['masks'][:,:,index].astype(np.uint8)\n",
    "                        contours = find_contours(mask, 0.5)\n",
    "                        contour = contours[0]\n",
    "                        contour = np.flip(contour, axis=1)\n",
    "                        contour = approximate_polygon(contour, tolerance=2.5)\n",
    "                        segmentation = contour.ravel().tolist()\n",
    "                        label = self.labels_mapping[c_id]\n",
    "                        if label not in result:\n",
    "                            result[label] = []\n",
    "                        result[label].append(segmentation)\n",
    "        return result\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def process_polygons(polygons, boxes):\n",
    "        \"\"\"\n",
    "           Check if any point of the polygon falls into any of coconot palms except for dead/non_recoverable.\n",
    "        \"\"\"\n",
    "        def _check_inside_boxes(polygon, boxes):\n",
    "            for point in polygon:\n",
    "                for label, bxes in boxes.items():\n",
    "                    for box in bxes:\n",
    "                        if point[0] > box[0] and point[0] < box[2] and point[1] > box[1] and point[1] < box[3] and label not in ['dead','non_recoverable']:\n",
    "                            # point is inside rectangle\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "        result = {}\n",
    "        for label_m, polys in polygons.items():\n",
    "            for polygon in polys:\n",
    "                p = [polygon[i:i+2] for i in range(0, len(polygon),2)]\n",
    "                if _check_inside_boxes(p, boxes):\n",
    "                    if label_m not in result:\n",
    "                        result[label_m] = []\n",
    "                    result[label_m].append(polygon)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def draw_instances(frame, boxes, masks):\n",
    "    colors = {'zero':(0,255,0), 'light':(0,0,255),'medium':(255,0,0),'high':(120,120,0),'non_recoverable':(0,120,120),'cut':(0,0,0)}\n",
    "    #draw boxes\n",
    "    for label, bxes in boxes.items():\n",
    "        for box in bxes:\n",
    "            cv2.rectangle(frame, (box[0],box[1]), (box[2],box[3]), colors[label], 5)\n",
    "    #draw polygons\n",
    "    for label, polygons in masks.items():\n",
    "        for polygon in polygons:\n",
    "            p = [polygon[i:i+2] for i in range(0, len(polygon),2)]\n",
    "            pts = np.array(p, np.int32)\n",
    "            pts = pts.reshape((-1,1,2))\n",
    "            cv2.polylines(frame, [pts], True, (0,255,255),5)\n",
    "    return frame\n",
    "\n",
    "def get_labels(classes_csv, type=\"od\"):\n",
    "    labels = []\n",
    "    with open(classes_csv, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "        # slogger.glob.info(\"class file data {}\".format(data))\n",
    "        for line in data[1:]:\n",
    "            if type == \"maskrcnn\":\n",
    "                if \",\" not in line:\n",
    "                    continue\n",
    "                # slogger.glob.info(\"classes line {}\".format(line))\n",
    "                label, num = line.strip().split(',')\n",
    "                labels.append(('label', [('name', line.strip())]))\n",
    "            else:\n",
    "                if \"label\" not in line:\n",
    "                    labels.append(('label', [('name', line.strip())]))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.type == \"both\":\n",
    "        od_model = ObjectDetection(args.od_model)\n",
    "        seg_model = Segmentation(args.mask_model)\n",
    "    elif args.type == \"classes\":\n",
    "        od_model = ObjectDetection(args.od_model)\n",
    "    elif args.type == \"v_shape\":\n",
    "        seg_model = Segmentation(args.mask_model)\n",
    "\n",
    "    cap = cv2.VideoCapture(args.video)\n",
    "    #would be better to take csv files as an input\n",
    "    #labels_mapping_od = {1:'dead', 2:'damaged',3:'healthy'}\n",
    "    labels_mapping_od = {1:'zero',2:'light',3:'medium',4:'high',5:'non_recoverable'}\n",
    "    frame_no = 0\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = math.ceil(cap.get(cv2.CAP_PROP_FPS))\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if args.num_frames == 'None' or args.num_frames == None:\n",
    "        args.num_frames = num_frames\n",
    "    #out = cv2.VideoWriter(os.path.basename(args.video)[:-4]+\"_skip_{}_numframes_{}.mp4\".format(args.skip_no, args.num_frames), fourcc, fps, (frame_width,frame_height))\n",
    "    # 'a' for annotation tacked onto end of filename\n",
    "    out = cv2.VideoWriter(f\"{args.output_dir}/{os.path.basename(args.video)[:-4]}a.mp4\", fourcc, fps, (frame_width,frame_height))\n",
    "\n",
    "    labels_from_csv = get_labels(args.classes_cvat, args.classes_type)\n",
    "    print(\"Labels: \", labels_from_csv)\n",
    "    final_result = {'meta':{'task': OrderedDict([('id',str(args.task_id)),('name',str(args.task_name)),('size',str(num_frames)),('mode','interpolation'),('start_frame', str(0)),('stop_frame', str(num_frames-1)),('z_order',\"False\"),('labels', labels_from_csv)])}, 'frames':[]}\n",
    "\n",
    "    #output_xml_path = \"cvat_annotation_\"+os.path.basename(args.video)[:-4]+\"_skip_{}_numframes_{}.xml\".format(args.skip_no, args.num_frames)\n",
    "    output_xml_path = f\"{args.output_dir}/{os.path.basename(args.video)[:-4]}.xml\"\t\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            if frame_no % args.skip_no != 0:\n",
    "                frame_no += 1\n",
    "                continue\n",
    "            print(\"Processing frame: \", frame_no)\n",
    "            # get image ready for inference\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_org = Image.fromarray(img)\n",
    "            width, height = image_org.size\n",
    "            if width > 1920 or height > 1080:\n",
    "                image = image_org.resize((width // 2, height // 2), Image.ANTIALIAS)\n",
    "            else:\n",
    "                image = image_org\n",
    "            image_np = load_image_into_numpy(image)\n",
    "            image_mask_rcnn = load_image_into_numpy(image_org)\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            od_result = {}\n",
    "            result = {}\n",
    "            if args.type == \"both\" or args.type == \"classes\":\n",
    "                # run detection\n",
    "                boxes, scores, classes, num_detections = od_model.get_detections(image_np_expanded)\n",
    "                #normalize bounding boxes, also apply threshold\n",
    "                od_result = ObjectDetection.process_boxes(boxes, scores, classes, labels_mapping_od, args.od_threshold, width, height)\n",
    "                if od_result:\n",
    "                    print(\"od\", od_result)\n",
    "                    shapes = []\n",
    "                    for label, boxes in od_result.items():\n",
    "                        for box in boxes:\n",
    "                            shapes.append({'type':'rectangle','label':label,'occluded':0,'points':box})\n",
    "                    final_result['frames'].append({'frame':frame_no, 'width':frame_width, 'height':frame_height, 'shapes':shapes})\n",
    "            if args.type == \"both\" or args.type == \"v_shape\":\n",
    "                # run segmentation\n",
    "                result = seg_model.get_polygons([image_mask_rcnn], args.mask_threshold)\n",
    "                print(\"Result without processing: \", result)\n",
    "                if args.type == \"both\" or args.type == \"classes\":\n",
    "                    # filter out false positives if boxes are available\n",
    "                    result = Segmentation.process_polygons(result, od_result)\n",
    "                print(\"Result: \", result)\n",
    "                if result:\n",
    "                    shapes = []\n",
    "                    for label, polygons in result.items():\n",
    "                        for polygon in polygons:\n",
    "                            shapes.append({'type':'polygon','label':label,'occluded':0,'points':polygon})\n",
    "                    frame_exists = False\n",
    "                    for frame_ in final_result['frames']:\n",
    "                        if frame_['frame'] == frame_no:\n",
    "                            break\n",
    "                    if frame_exists:\n",
    "                        final_result['frames']['shapes'].extend(shapes)\n",
    "                    else:\n",
    "                        final_result['frames'].append({'frame':frame_no, 'width':frame_width, 'height':frame_height, 'shapes':shapes})\n",
    "\n",
    "            frame = draw_instances(frame, od_result, result)\n",
    "            #write video\n",
    "            out.write(frame)\n",
    "\n",
    "            if (frame_no // args.skip_no) + 1 == int(args.num_frames):\n",
    "                dump_as_cvat_annotation(open(output_xml_path,\"w\"), final_result)\n",
    "                cap.release()\n",
    "                out.release()\n",
    "                break\n",
    "            frame_no += 1\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "\n",
    "                print(\"Final result: \", final_result)\n",
    "                dump_as_cvat_annotation(open(output_xml_path, \"w\"), final_result)\n",
    "                cap.release()\n",
    "                out.release()\n",
    "                break\n",
    "            except:  #handle case when video is corrupted or does not exists\n",
    "                break\n",
    "\n",
    "    return output_xml_path, args.num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  [('label', [('name', 'zero')]), ('label', [('name', 'light')]), ('label', [('name', 'medium')]), ('label', [('name', 'high')]), ('label', [('name', 'non_recoverable')])]\n",
      "Processing frame:  0\n",
      "od {'light': [[528, 827, 681, 1045]]}\n",
      "Result without processing:  {}\n",
      "Result:  {}\n",
      "Processing frame:  5\n",
      "od {'zero': [[593, 812, 761, 1043]]}\n",
      "Result without processing:  {}\n",
      "Result:  {}\n",
      "Processing frame:  10\n",
      "od {'light': [[938, 834, 1120, 1059], [671, 799, 852, 1033]], 'zero': [[664, 801, 856, 1030]]}\n",
      "Result without processing:  {}\n",
      "Result:  {}\n",
      "Processing frame:  15\n",
      "od {'light': [[1023, 821, 1224, 1045], [747, 787, 944, 1045]]}\n",
      "Result without processing:  {}\n",
      "Result:  {}\n",
      "Processing frame:  20\n",
      "od {'light': [[1129, 805, 1351, 1030]]}\n",
      "Result without processing:  {}\n",
      "Result:  {}\n"
     ]
    }
   ],
   "source": [
    "# aws s3 cp s3://cnas-re.uog.onepanel.io/raw-input/20200703/20200703_121802.geojson inference_data/\n",
    "# aws s3 cp s3://cnas-re.uog.onepanel.io/raw-input/20200703/20200703_121802.mp4 inference_data/\n",
    "# aws s3 cp s3://cnas-re.uog.onepanel.io/raw-input/20200703/20200703_121802_gps.csv inference_data/\n",
    "\n",
    "# python3 demo.py --num_frames 1000\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--type\",default=\"both\",help=\"what type of models to use [both,classes,v_shape]\")\n",
    "    parser.add_argument(\"--video\", default=\"/home/aubreytensor1/Guam02/rawdata/20201211_141545.mp4\", help=\"path to video\")\n",
    "    parser.add_argument(\"--gps_csv\", default=\"inference_data/20200703_121802_gps.csv\", help=\"path to csv containing gps data\")\n",
    "    parser.add_argument(\"--skip_no\", default=5, type=int, help=\"num of frames to skip\")\n",
    "    parser.add_argument(\"--num_frames\", default=5, help=\"how many frames to consider?\")\n",
    "    parser.add_argument(\"--od_model\", default=\"inference_data/frozen_inference_graph_5classes.pb\" , help=\"path to trained detection model\")\n",
    "    parser.add_argument(\"--classes_cvat\", default=\"inference_data/5classes.csv\", help=\"classes you want to use for cvat, see readme for more details.\")\n",
    "    parser.add_argument(\"--classes_type\", default=\"od\", help=\"type of classes csv file [od, maskrcnn]\")\n",
    "    parser.add_argument(\"--mask_model\", default=\"inference_data/mask_rcnn_cvat_0160.h5\", help=\"path to trained maskrcnn model\")\n",
    "    parser.add_argument(\"--od_threshold\",type=float, default=0.5, help=\"threshold for IoU\")\n",
    "    parser.add_argument(\"--mask_threshold\",type=float, default=0.5, help=\"threshold for maskrcnn\")\n",
    "    parser.add_argument(\"--output_video\", default=\"output.mp4\", help=\"where to store output video\")\n",
    "    parser.add_argument(\"--survey_type\", default=\"v_shape\",help=\"what to write in geojson [v_shape,classes\")\n",
    "    parser.add_argument(\"--task_id\", default=0, type=int, help=\"required only if you want to use this in cvat\")\n",
    "    parser.add_argument(\"--task_name\", default=\"demo\", help=\"requierd only if you want to use this in cvat\")\n",
    "    parser.add_argument(\"--write_into_objects\", default=True, help=\"should this enter detected objects into objects table?\")\n",
    "    parser.add_argument(\"--drop_extra_clm\", default=True, help=\"whether it should drop extra columns? required if dumping into objects table\")\n",
    "    parser.add_argument(\"--output_dir\", default='/home/aubreytensor1/Guam02/testoutput', help=\"output directory for video with objects detected and CVAT xml\") \n",
    "    # have to use string as we cant have condition statements in workflow\n",
    "    parser.add_argument('--dump_sql', default=False)\n",
    "    #args = parser.parse_args()\n",
    "    \n",
    "    # FOR TESTING IN A JUPYTER NOTEBOOK, SET DEFAULTS AND UNCOMMENT THE FOLLOWING LINE\n",
    "    args = parser.parse_args(\"\")   \n",
    "        \n",
    "    if args.type not in ['both','classes','v_shape']:\n",
    "        raise ValueError('Invalid type: {}. Valid options are \"both\",\"classes\",\"v_shape\".'.format(args.type))\n",
    "    # if not os.path.exists(args.video):\n",
    "    #     raise FileExistsError(\"Video does not exist!\")\n",
    "    output_xml_path, num_frames_ = main(args)\n",
    "    if args.dump_sql == True or args.dump_sql == 'True':\n",
    "        dump_to_sql(output_xml_path, args.gps_csv, os.path.basename(args.video), args.skip_no, args.write_into_objects, args.drop_extra_clm, num_frames_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cmd = b'--dump_sql False --video ~/Guam02/rawdata/20201211_141545.mp4 --num_frames 5 --skip_no 5 --output_dir ~/Guam02/testoutput'\n",
    "\n",
    "args = parser.parse_args(cmd)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes_cvat': 'inference_data/5classes.csv',\n",
       " 'classes_type': 'od',\n",
       " 'drop_extra_clm': True,\n",
       " 'dump_sql': False,\n",
       " 'gps_csv': 'inference_data/20200703_121802_gps.csv',\n",
       " 'mask_model': 'inference_data/mask_rcnn_cvat_0160.h5',\n",
       " 'mask_threshold': 0.5,\n",
       " 'num_frames': 5,\n",
       " 'od_model': 'inference_data/frozen_inference_graph_5classes.pb',\n",
       " 'od_threshold': 0.5,\n",
       " 'output_dir': '~/Guam02/testoutput',\n",
       " 'output_video': 'output.mp4',\n",
       " 'skip_no': 5,\n",
       " 'survey_type': 'v_shape',\n",
       " 'task_id': 0,\n",
       " 'task_name': 'demo',\n",
       " 'type': 'both',\n",
       " 'video': '~/Guam02/rawdata/20201211_141545.mp4',\n",
       " 'write_into_objects': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
