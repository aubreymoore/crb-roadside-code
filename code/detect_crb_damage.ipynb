{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chief-forwarding",
   "metadata": {},
   "source": [
    "# detect_crb_damage.ipynb\n",
    "\n",
    "**If a ResourceExhausteError is returned when firing up the GPU, this can usually be handled by restarting the machine.**\n",
    "\n",
    "* 2021-05-02 First version by Aubrey Moore\n",
    "\n",
    "This notebook uses a pair of Tensorflow object detectors to measure coconut rhinoceros beetle damage in digital images.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "    papermill detect_crb_damage.ipynb \\\n",
    "     '../open-camera-test/home-uog/detect_crb_damage_output.ipynb' \\\n",
    "    -p IMAGE_FILE_PATH '../rawdata/*.jpg' \\\n",
    "    -p OUTPUT_XML_PATH '../output/detected_objects.xml'\n",
    "\n",
    "When the above command line is executed in the directory containing **detect_crb_damage.ipynb**, \n",
    "all **jpg** image files in the **IMAGE_FILE_PATH** directory will be scanned by the\n",
    "object detectors and results will be saved in **OUTPUT_XML_PATH**.\n",
    "\n",
    "2022-07-23 Added MAX_IMAGES for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formal-briefing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aubreytensor1/.local/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aubreytensor1/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# import tensorflow as tf\n",
    "# uncomment following lines if you are using TF2\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "negative-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import cv2\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append('Mask_RCNN')\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.model as modellib\n",
    "\n",
    "from xml_dumper import dump_as_cvat_annotation\n",
    "import skimage.io\n",
    "from collections import OrderedDict\n",
    "from skimage.measure import find_contours, approximate_polygon\n",
    "import glob\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "duplicate-phoenix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rental-consolidation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 329 µs (started: 2022-09-04 06:52:41 +10:00)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deluxe-dollar",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 822 µs (started: 2022-09-04 06:52:41 +10:00)\n"
     ]
    }
   ],
   "source": [
    "# parameters for papermill\n",
    "\n",
    "IMAGE_FILE_PATH = '../rawdata/*.jpg' # Path to one or more image files. Can include wildcards. See https://pymotw.com/2/glob/ for pattern matching details.\n",
    "OUTPUT_XML_PATH = '../output/detected_objects.xml' # Path to output file which will contain metadata for detected objects.\n",
    "MAX_IMAGES = 1000000  # maximum number of images to be processed\n",
    "\n",
    "TYPE = 'both' # what type of models to use [both,classes,v_shape]\n",
    "#SKIP_NO = 1 # int, num of frames to skip (must be >0)\n",
    "#NUM_FRAMES = None # how many frames to consider?\n",
    "OD_MODEL = \"object-detectors/inference_data/frozen_inference_graph_5classes.pb\" # path to trained detection model\n",
    "CLASSES_CVAT = \"object-detectors/inference_data/5classes.csv\" # classes you want to use for cvat, see readme for more details.\n",
    "CLASSES_TYPE = \"od\" # type of classes csv file [od, maskrcnn]\n",
    "MASK_MODEL =  \"object-detectors/inference_data/mask_rcnn_cvat_0160.h5\" # path to trained maskrcnn model\n",
    "OD_THRESHOLD = 0.5 # threshold for IoU\n",
    "MASK_THRESHOLD = 0.5 # threshold for maskrcnn\n",
    "#SURVEY_TYPE = \"v_shape\" # what to write in geojson [v_shape,classes]\n",
    "TASK_ID = 0 # required only if you want to use this in cvat\n",
    "TASK_NAME = \"demo\" # required only if you want to use this in cvat\n",
    "DUMP_SQL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rotary-lucas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.13 ms (started: 2022-09-04 06:52:41 +10:00)\n"
     ]
    }
   ],
   "source": [
    "class ObjectDetection:\n",
    "    def __init__(self, model_path):\n",
    "        self.detection_graph = tf.Graph()\n",
    "        with self.detection_graph.as_default():\n",
    "            od_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(model_path , 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "                config = tf.ConfigProto()\n",
    "                config.gpu_options.allow_growth=True\n",
    "                self.sess = tf.Session(graph=self.detection_graph, config=config)\n",
    "\n",
    "    def get_detections(self, image_np_expanded):\n",
    "        image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        (boxes, scores, classes, num_detections) = self.sess.run([boxes, scores, classes, num_detections], feed_dict={image_tensor: image_np_expanded})\n",
    "        return boxes, scores, classes, num_detections\n",
    "\n",
    "    @staticmethod\n",
    "    def process_boxes(boxes, scores, classes, labels_mapping, threshold, width, height):\n",
    "        result = {}\n",
    "        for i in range(len(classes[0])):\n",
    "            if classes[0][i] in labels_mapping.keys():\n",
    "                if scores[0][i] >= threshold:\n",
    "                    xmin = int(boxes[0][i][1] * width)\n",
    "                    ymin = int(boxes[0][i][0] * height)\n",
    "                    xmax = int(boxes[0][i][3] * width)\n",
    "                    ymax = int(boxes[0][i][2] * height)\n",
    "                    label = labels_mapping[classes[0][i]]\n",
    "                    if label not in result:\n",
    "                        result[label] = []\n",
    "                    result[label].append([xmin,ymin,xmax,ymax])\n",
    "        return result\n",
    "\n",
    "class Segmentation:\n",
    "    def __init__(self, model_path, num_c=2):\n",
    "        class InferenceConfig(Config):\n",
    "            # Set batch size to 1 since we'll be running inference on\n",
    "            # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "            NAME = \"cvat\"\n",
    "            GPU_COUNT = 1\n",
    "            IMAGES_PER_GPU = 1\n",
    "            NUM_CLASSES = num_c\n",
    "\n",
    "        config = InferenceConfig()\n",
    "        #config.display()\n",
    "\n",
    "        # Create model object in inference mode.\n",
    "        self.model = modellib.MaskRCNN(mode=\"inference\", model_dir=\"./output\", config=config)\n",
    "        # Load weights trained on MS-COCO\n",
    "        self.model.load_weights(model_path, by_name=True)\n",
    "        self.labels_mapping = {0:'BG', 1:'cut'}\n",
    "\n",
    "    def get_polygons(self, images, threshold):\n",
    "        res = self.model.detect(images)\n",
    "        result = {}\n",
    "        for r in res:\n",
    "            for index, c_id in enumerate(r['class_ids']):\n",
    "                if c_id in self.labels_mapping.keys():\n",
    "                    if r['scores'][index] >= threshold:\n",
    "                        mask = r['masks'][:,:,index].astype(np.uint8)\n",
    "                        contours = find_contours(mask, 0.5)\n",
    "\n",
    "                        # KLUDGE\n",
    "                        # Handles a rare \"list index out of range error\" for contours[0]\n",
    "                        # If the contours array is empty, a dummy contour consisting of\n",
    "                        # the top left pisxel is provided.\n",
    "\n",
    "                        if not contours:\n",
    "                            print('ERROR: contour list is empty.')\n",
    "                            contour = np.array([[1.0,1.0],[1.0,0.0],[0.0,0.0],[0.0,1.0],[1.0,1.0]])\n",
    "                        else:\n",
    "                            contour = contours[0]\n",
    "                            # print(f'contour ({type(contour)}): {contour}')\n",
    "\n",
    "                        # end of KLUDGE\n",
    "\n",
    "                        contour = np.flip(contour, axis=1)\n",
    "                        contour = approximate_polygon(contour, tolerance=2.5)\n",
    "                        segmentation = contour.ravel().tolist()\n",
    "                        label = self.labels_mapping[c_id]\n",
    "                        if label not in result:\n",
    "                            result[label] = []\n",
    "                        result[label].append(segmentation)\n",
    "        return result\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def process_polygons(polygons, boxes):\n",
    "        \"\"\"\n",
    "           Check if any point of the polygon falls into any of coconot palms except for dead/non_recoverable.\n",
    "        \"\"\"\n",
    "        def _check_inside_boxes(polygon, boxes):\n",
    "            for point in polygon:\n",
    "                for label, bxes in boxes.items():\n",
    "                    for box in bxes:\n",
    "                        if point[0] > box[0] and point[0] < box[2] and point[1] > box[1] and point[1] < box[3] and label not in ['dead','non_recoverable']:\n",
    "                            # point is inside rectangle\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "        result = {}\n",
    "        for label_m, polys in polygons.items():\n",
    "            for polygon in polys:\n",
    "                p = [polygon[i:i+2] for i in range(0, len(polygon),2)]\n",
    "                if _check_inside_boxes(p, boxes):\n",
    "                    if label_m not in result:\n",
    "                        result[label_m] = []\n",
    "                    result[label_m].append(polygon)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def load_image_into_numpy(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def draw_instances(frame, boxes, masks):\n",
    "    colors = {'zero':(0,255,0), 'light':(0,0,255),'medium':(255,0,0),'high':(120,120,0),'non_recoverable':(0,120,120),'cut':(0,0,0)}\n",
    "    #draw boxes\n",
    "    for label, bxes in boxes.items():\n",
    "        for box in bxes:\n",
    "            cv2.rectangle(frame, (box[0],box[1]), (box[2],box[3]), colors[label], 5)\n",
    "    #draw polygons\n",
    "    for label, polygons in masks.items():\n",
    "        for polygon in polygons:\n",
    "            p = [polygon[i:i+2] for i in range(0, len(polygon),2)]\n",
    "            pts = np.array(p, np.int32)\n",
    "            pts = pts.reshape((-1,1,2))\n",
    "            cv2.polylines(frame, [pts], True, (0,255,255),5)\n",
    "    return frame\n",
    "\n",
    "def get_labels(classes_csv, type=\"od\"):\n",
    "    labels = []\n",
    "    with open(classes_csv, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "        # slogger.glob.info(\"class file data {}\".format(data))\n",
    "        for line in data[1:]:\n",
    "            if type == \"maskrcnn\":\n",
    "                if \",\" not in line:\n",
    "                    continue\n",
    "                # slogger.glob.info(\"classes line {}\".format(line))\n",
    "                label, num = line.strip().split(',')\n",
    "                labels.append(('label', [('name', line.strip())]))\n",
    "            else:\n",
    "                if \"label\" not in line:\n",
    "                    labels.append(('label', [('name', line.strip())]))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "front-republican",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:41+1000 [INFO] <module> Starting detect_crb_damage.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:43+1000 [WARNING] __getattr__ From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Mask_RCNN/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:45+1000 [WARNING] __getattr__ From Mask_RCNN/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Mask_RCNN/mrcnn/model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:45+1000 [WARNING] new_func From Mask_RCNN/mrcnn/model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Mask_RCNN/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:45+1000 [WARNING] new_func From Mask_RCNN/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Mask_RCNN/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:45+1000 [WARNING] __getattr__ From Mask_RCNN/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Mask_RCNN/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:45+1000 [WARNING] __getattr__ From Mask_RCNN/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Mask_RCNN/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:46+1000 [WARNING] new_func From Mask_RCNN/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04T06:52:51+1000 [WARNING] __getattr__ From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2022-09-04T06:58:24+1000 [INFO] <module> Image 100 of 23842\n",
      "2022-09-04T07:04:00+1000 [INFO] <module> Image 200 of 23842\n",
      "2022-09-04T07:09:37+1000 [INFO] <module> Image 300 of 23842\n",
      "2022-09-04T07:15:13+1000 [INFO] <module> Image 400 of 23842\n",
      "2022-09-04T07:20:49+1000 [INFO] <module> Image 500 of 23842\n",
      "2022-09-04T07:26:25+1000 [INFO] <module> Image 600 of 23842\n",
      "2022-09-04T07:32:02+1000 [INFO] <module> Image 700 of 23842\n",
      "2022-09-04T07:37:39+1000 [INFO] <module> Image 800 of 23842\n",
      "2022-09-04T07:43:15+1000 [INFO] <module> Image 900 of 23842\n",
      "2022-09-04T07:48:52+1000 [INFO] <module> Image 1000 of 23842\n",
      "2022-09-04T07:54:28+1000 [INFO] <module> Image 1100 of 23842\n",
      "2022-09-04T08:00:05+1000 [INFO] <module> Image 1200 of 23842\n",
      "2022-09-04T08:05:40+1000 [INFO] <module> Image 1300 of 23842\n",
      "2022-09-04T08:11:18+1000 [INFO] <module> Image 1400 of 23842\n",
      "2022-09-04T08:16:55+1000 [INFO] <module> Image 1500 of 23842\n",
      "2022-09-04T08:22:33+1000 [INFO] <module> Image 1600 of 23842\n",
      "2022-09-04T08:28:10+1000 [INFO] <module> Image 1700 of 23842\n",
      "2022-09-04T08:33:47+1000 [INFO] <module> Image 1800 of 23842\n",
      "2022-09-04T08:39:24+1000 [INFO] <module> Image 1900 of 23842\n",
      "2022-09-04T08:45:02+1000 [INFO] <module> Image 2000 of 23842\n",
      "2022-09-04T08:50:39+1000 [INFO] <module> Image 2100 of 23842\n",
      "2022-09-04T08:56:16+1000 [INFO] <module> Image 2200 of 23842\n",
      "2022-09-04T09:01:53+1000 [INFO] <module> Image 2300 of 23842\n",
      "2022-09-04T09:07:30+1000 [INFO] <module> Image 2400 of 23842\n",
      "2022-09-04T09:13:07+1000 [INFO] <module> Image 2500 of 23842\n",
      "2022-09-04T09:18:43+1000 [INFO] <module> Image 2600 of 23842\n",
      "2022-09-04T09:24:19+1000 [INFO] <module> Image 2700 of 23842\n",
      "2022-09-04T09:29:55+1000 [INFO] <module> Image 2800 of 23842\n",
      "2022-09-04T09:35:31+1000 [INFO] <module> Image 2900 of 23842\n",
      "2022-09-04T09:41:06+1000 [INFO] <module> Image 3000 of 23842\n",
      "2022-09-04T09:46:42+1000 [INFO] <module> Image 3100 of 23842\n",
      "2022-09-04T09:52:18+1000 [INFO] <module> Image 3200 of 23842\n",
      "2022-09-04T09:57:55+1000 [INFO] <module> Image 3300 of 23842\n",
      "2022-09-04T10:03:32+1000 [INFO] <module> Image 3400 of 23842\n",
      "2022-09-04T10:09:09+1000 [INFO] <module> Image 3500 of 23842\n",
      "2022-09-04T10:14:45+1000 [INFO] <module> Image 3600 of 23842\n",
      "2022-09-04T10:20:22+1000 [INFO] <module> Image 3700 of 23842\n",
      "2022-09-04T10:25:59+1000 [INFO] <module> Image 3800 of 23842\n",
      "2022-09-04T10:31:35+1000 [INFO] <module> Image 3900 of 23842\n",
      "2022-09-04T10:37:12+1000 [INFO] <module> Image 4000 of 23842\n",
      "2022-09-04T10:42:49+1000 [INFO] <module> Image 4100 of 23842\n",
      "2022-09-04T10:48:25+1000 [INFO] <module> Image 4200 of 23842\n",
      "2022-09-04T10:54:02+1000 [INFO] <module> Image 4300 of 23842\n",
      "2022-09-04T10:59:38+1000 [INFO] <module> Image 4400 of 23842\n",
      "2022-09-04T11:05:15+1000 [INFO] <module> Image 4500 of 23842\n",
      "2022-09-04T11:10:55+1000 [INFO] <module> Image 4600 of 23842\n",
      "2022-09-04T11:16:32+1000 [INFO] <module> Image 4700 of 23842\n",
      "2022-09-04T11:22:09+1000 [INFO] <module> Image 4800 of 23842\n",
      "2022-09-04T11:27:48+1000 [INFO] <module> Image 4900 of 23842\n",
      "2022-09-04T11:33:23+1000 [INFO] <module> Image 5000 of 23842\n",
      "2022-09-04T11:38:59+1000 [INFO] <module> Image 5100 of 23842\n",
      "2022-09-04T11:44:35+1000 [INFO] <module> Image 5200 of 23842\n",
      "2022-09-04T11:50:12+1000 [INFO] <module> Image 5300 of 23842\n",
      "2022-09-04T11:55:49+1000 [INFO] <module> Image 5400 of 23842\n",
      "2022-09-04T12:01:25+1000 [INFO] <module> Image 5500 of 23842\n",
      "2022-09-04T12:07:00+1000 [INFO] <module> Image 5600 of 23842\n",
      "2022-09-04T12:12:36+1000 [INFO] <module> Image 5700 of 23842\n",
      "2022-09-04T12:18:11+1000 [INFO] <module> Image 5800 of 23842\n",
      "2022-09-04T12:23:48+1000 [INFO] <module> Image 5900 of 23842\n",
      "2022-09-04T12:29:24+1000 [INFO] <module> Image 6000 of 23842\n",
      "2022-09-04T12:35:00+1000 [INFO] <module> Image 6100 of 23842\n",
      "2022-09-04T12:40:36+1000 [INFO] <module> Image 6200 of 23842\n",
      "2022-09-04T12:46:13+1000 [INFO] <module> Image 6300 of 23842\n",
      "2022-09-04T12:51:49+1000 [INFO] <module> Image 6400 of 23842\n",
      "2022-09-04T12:57:25+1000 [INFO] <module> Image 6500 of 23842\n",
      "2022-09-04T13:03:03+1000 [INFO] <module> Image 6600 of 23842\n",
      "2022-09-04T13:08:40+1000 [INFO] <module> Image 6700 of 23842\n",
      "2022-09-04T13:14:17+1000 [INFO] <module> Image 6800 of 23842\n",
      "2022-09-04T13:19:53+1000 [INFO] <module> Image 6900 of 23842\n",
      "2022-09-04T13:25:30+1000 [INFO] <module> Image 7000 of 23842\n",
      "2022-09-04T13:31:06+1000 [INFO] <module> Image 7100 of 23842\n",
      "2022-09-04T13:36:43+1000 [INFO] <module> Image 7200 of 23842\n",
      "2022-09-04T13:42:19+1000 [INFO] <module> Image 7300 of 23842\n",
      "2022-09-04T13:47:56+1000 [INFO] <module> Image 7400 of 23842\n",
      "2022-09-04T13:53:32+1000 [INFO] <module> Image 7500 of 23842\n",
      "2022-09-04T13:59:08+1000 [INFO] <module> Image 7600 of 23842\n",
      "2022-09-04T14:04:45+1000 [INFO] <module> Image 7700 of 23842\n",
      "2022-09-04T14:10:21+1000 [INFO] <module> Image 7800 of 23842\n",
      "2022-09-04T14:15:58+1000 [INFO] <module> Image 7900 of 23842\n",
      "2022-09-04T14:21:35+1000 [INFO] <module> Image 8000 of 23842\n",
      "2022-09-04T14:27:14+1000 [INFO] <module> Image 8100 of 23842\n",
      "2022-09-04T14:32:51+1000 [INFO] <module> Image 8200 of 23842\n",
      "2022-09-04T14:38:27+1000 [INFO] <module> Image 8300 of 23842\n",
      "2022-09-04T14:44:05+1000 [INFO] <module> Image 8400 of 23842\n",
      "2022-09-04T14:49:42+1000 [INFO] <module> Image 8500 of 23842\n",
      "2022-09-04T14:55:19+1000 [INFO] <module> Image 8600 of 23842\n",
      "2022-09-04T15:00:56+1000 [INFO] <module> Image 8700 of 23842\n",
      "2022-09-04T15:06:33+1000 [INFO] <module> Image 8800 of 23842\n",
      "2022-09-04T15:12:10+1000 [INFO] <module> Image 8900 of 23842\n",
      "2022-09-04T15:17:48+1000 [INFO] <module> Image 9000 of 23842\n",
      "2022-09-04T15:23:26+1000 [INFO] <module> Image 9100 of 23842\n",
      "2022-09-04T15:29:03+1000 [INFO] <module> Image 9200 of 23842\n",
      "2022-09-04T15:34:39+1000 [INFO] <module> Image 9300 of 23842\n",
      "2022-09-04T15:40:15+1000 [INFO] <module> Image 9400 of 23842\n",
      "2022-09-04T15:45:51+1000 [INFO] <module> Image 9500 of 23842\n",
      "2022-09-04T15:51:28+1000 [INFO] <module> Image 9600 of 23842\n",
      "2022-09-04T15:57:05+1000 [INFO] <module> Image 9700 of 23842\n",
      "2022-09-04T16:02:42+1000 [INFO] <module> Image 9800 of 23842\n",
      "2022-09-04T16:08:19+1000 [INFO] <module> Image 9900 of 23842\n",
      "2022-09-04T16:13:56+1000 [INFO] <module> Image 10000 of 23842\n",
      "2022-09-04T16:19:33+1000 [INFO] <module> Image 10100 of 23842\n",
      "2022-09-04T16:25:10+1000 [INFO] <module> Image 10200 of 23842\n",
      "2022-09-04T16:30:46+1000 [INFO] <module> Image 10300 of 23842\n",
      "2022-09-04T16:36:23+1000 [INFO] <module> Image 10400 of 23842\n",
      "2022-09-04T16:41:59+1000 [INFO] <module> Image 10500 of 23842\n",
      "2022-09-04T16:47:36+1000 [INFO] <module> Image 10600 of 23842\n",
      "2022-09-04T16:53:13+1000 [INFO] <module> Image 10700 of 23842\n",
      "2022-09-04T16:58:50+1000 [INFO] <module> Image 10800 of 23842\n",
      "2022-09-04T17:04:26+1000 [INFO] <module> Image 10900 of 23842\n",
      "2022-09-04T17:10:02+1000 [INFO] <module> Image 11000 of 23842\n",
      "2022-09-04T17:15:37+1000 [INFO] <module> Image 11100 of 23842\n",
      "2022-09-04T17:21:13+1000 [INFO] <module> Image 11200 of 23842\n",
      "2022-09-04T17:26:50+1000 [INFO] <module> Image 11300 of 23842\n",
      "2022-09-04T17:32:26+1000 [INFO] <module> Image 11400 of 23842\n",
      "2022-09-04T17:38:02+1000 [INFO] <module> Image 11500 of 23842\n",
      "2022-09-04T17:43:38+1000 [INFO] <module> Image 11600 of 23842\n",
      "2022-09-04T17:49:15+1000 [INFO] <module> Image 11700 of 23842\n",
      "2022-09-04T17:54:51+1000 [INFO] <module> Image 11800 of 23842\n",
      "2022-09-04T18:00:28+1000 [INFO] <module> Image 11900 of 23842\n",
      "2022-09-04T18:06:05+1000 [INFO] <module> Image 12000 of 23842\n",
      "2022-09-04T18:11:41+1000 [INFO] <module> Image 12100 of 23842\n",
      "2022-09-04T18:17:18+1000 [INFO] <module> Image 12200 of 23842\n",
      "2022-09-04T18:22:53+1000 [INFO] <module> Image 12300 of 23842\n",
      "2022-09-04T18:28:30+1000 [INFO] <module> Image 12400 of 23842\n",
      "2022-09-04T18:34:06+1000 [INFO] <module> Image 12500 of 23842\n",
      "2022-09-04T18:39:42+1000 [INFO] <module> Image 12600 of 23842\n",
      "2022-09-04T18:45:20+1000 [INFO] <module> Image 12700 of 23842\n",
      "2022-09-04T18:50:56+1000 [INFO] <module> Image 12800 of 23842\n",
      "2022-09-04T18:56:33+1000 [INFO] <module> Image 12900 of 23842\n",
      "2022-09-04T19:02:09+1000 [INFO] <module> Image 13000 of 23842\n",
      "2022-09-04T19:07:45+1000 [INFO] <module> Image 13100 of 23842\n",
      "2022-09-04T19:13:21+1000 [INFO] <module> Image 13200 of 23842\n",
      "2022-09-04T19:18:57+1000 [INFO] <module> Image 13300 of 23842\n",
      "2022-09-04T19:24:33+1000 [INFO] <module> Image 13400 of 23842\n",
      "2022-09-04T19:30:09+1000 [INFO] <module> Image 13500 of 23842\n",
      "2022-09-04T19:35:45+1000 [INFO] <module> Image 13600 of 23842\n",
      "2022-09-04T19:41:21+1000 [INFO] <module> Image 13700 of 23842\n",
      "2022-09-04T19:46:57+1000 [INFO] <module> Image 13800 of 23842\n",
      "2022-09-04T19:52:34+1000 [INFO] <module> Image 13900 of 23842\n",
      "2022-09-04T19:58:10+1000 [INFO] <module> Image 14000 of 23842\n",
      "2022-09-04T20:03:46+1000 [INFO] <module> Image 14100 of 23842\n",
      "2022-09-04T20:09:21+1000 [INFO] <module> Image 14200 of 23842\n",
      "2022-09-04T20:14:57+1000 [INFO] <module> Image 14300 of 23842\n",
      "2022-09-04T20:20:33+1000 [INFO] <module> Image 14400 of 23842\n",
      "2022-09-04T20:26:08+1000 [INFO] <module> Image 14500 of 23842\n",
      "2022-09-04T20:31:45+1000 [INFO] <module> Image 14600 of 23842\n",
      "2022-09-04T20:37:21+1000 [INFO] <module> Image 14700 of 23842\n",
      "2022-09-04T20:42:58+1000 [INFO] <module> Image 14800 of 23842\n",
      "2022-09-04T20:48:34+1000 [INFO] <module> Image 14900 of 23842\n",
      "2022-09-04T20:54:11+1000 [INFO] <module> Image 15000 of 23842\n",
      "2022-09-04T20:59:49+1000 [INFO] <module> Image 15100 of 23842\n",
      "2022-09-04T21:05:26+1000 [INFO] <module> Image 15200 of 23842\n",
      "2022-09-04T21:11:01+1000 [INFO] <module> Image 15300 of 23842\n",
      "2022-09-04T21:16:38+1000 [INFO] <module> Image 15400 of 23842\n",
      "2022-09-04T21:22:14+1000 [INFO] <module> Image 15500 of 23842\n",
      "2022-09-04T21:27:51+1000 [INFO] <module> Image 15600 of 23842\n",
      "2022-09-04T21:33:26+1000 [INFO] <module> Image 15700 of 23842\n",
      "2022-09-04T21:39:02+1000 [INFO] <module> Image 15800 of 23842\n",
      "2022-09-04T21:44:39+1000 [INFO] <module> Image 15900 of 23842\n",
      "2022-09-04T21:50:15+1000 [INFO] <module> Image 16000 of 23842\n",
      "2022-09-04T21:55:51+1000 [INFO] <module> Image 16100 of 23842\n",
      "2022-09-04T22:01:27+1000 [INFO] <module> Image 16200 of 23842\n",
      "2022-09-04T22:07:03+1000 [INFO] <module> Image 16300 of 23842\n",
      "2022-09-04T22:12:39+1000 [INFO] <module> Image 16400 of 23842\n",
      "2022-09-04T22:18:15+1000 [INFO] <module> Image 16500 of 23842\n",
      "2022-09-04T22:23:51+1000 [INFO] <module> Image 16600 of 23842\n",
      "2022-09-04T22:29:28+1000 [INFO] <module> Image 16700 of 23842\n",
      "2022-09-04T22:35:05+1000 [INFO] <module> Image 16800 of 23842\n",
      "2022-09-04T22:40:41+1000 [INFO] <module> Image 16900 of 23842\n",
      "2022-09-04T22:46:16+1000 [INFO] <module> Image 17000 of 23842\n",
      "2022-09-04T22:51:52+1000 [INFO] <module> Image 17100 of 23842\n",
      "2022-09-04T22:57:28+1000 [INFO] <module> Image 17200 of 23842\n",
      "2022-09-04T23:03:04+1000 [INFO] <module> Image 17300 of 23842\n",
      "2022-09-04T23:08:39+1000 [INFO] <module> Image 17400 of 23842\n",
      "2022-09-04T23:14:15+1000 [INFO] <module> Image 17500 of 23842\n",
      "2022-09-04T23:19:51+1000 [INFO] <module> Image 17600 of 23842\n",
      "2022-09-04T23:25:27+1000 [INFO] <module> Image 17700 of 23842\n",
      "2022-09-04T23:31:02+1000 [INFO] <module> Image 17800 of 23842\n",
      "2022-09-04T23:36:38+1000 [INFO] <module> Image 17900 of 23842\n",
      "2022-09-04T23:42:14+1000 [INFO] <module> Image 18000 of 23842\n",
      "2022-09-04T23:47:49+1000 [INFO] <module> Image 18100 of 23842\n",
      "2022-09-04T23:53:25+1000 [INFO] <module> Image 18200 of 23842\n",
      "2022-09-04T23:59:01+1000 [INFO] <module> Image 18300 of 23842\n",
      "2022-09-05T00:04:36+1000 [INFO] <module> Image 18400 of 23842\n",
      "2022-09-05T00:10:12+1000 [INFO] <module> Image 18500 of 23842\n",
      "2022-09-05T00:15:48+1000 [INFO] <module> Image 18600 of 23842\n",
      "2022-09-05T00:21:24+1000 [INFO] <module> Image 18700 of 23842\n",
      "2022-09-05T00:26:59+1000 [INFO] <module> Image 18800 of 23842\n",
      "2022-09-05T00:32:34+1000 [INFO] <module> Image 18900 of 23842\n",
      "2022-09-05T00:38:09+1000 [INFO] <module> Image 19000 of 23842\n",
      "2022-09-05T00:43:43+1000 [INFO] <module> Image 19100 of 23842\n",
      "2022-09-05T00:49:19+1000 [INFO] <module> Image 19200 of 23842\n",
      "2022-09-05T00:54:54+1000 [INFO] <module> Image 19300 of 23842\n",
      "2022-09-05T01:00:30+1000 [INFO] <module> Image 19400 of 23842\n",
      "2022-09-05T01:06:05+1000 [INFO] <module> Image 19500 of 23842\n",
      "2022-09-05T01:11:42+1000 [INFO] <module> Image 19600 of 23842\n",
      "2022-09-05T01:17:18+1000 [INFO] <module> Image 19700 of 23842\n",
      "2022-09-05T01:22:53+1000 [INFO] <module> Image 19800 of 23842\n",
      "2022-09-05T01:28:29+1000 [INFO] <module> Image 19900 of 23842\n",
      "2022-09-05T01:34:04+1000 [INFO] <module> Image 20000 of 23842\n",
      "2022-09-05T01:39:50+1000 [INFO] <module> Image 20100 of 23842\n",
      "2022-09-05T01:45:25+1000 [INFO] <module> Image 20200 of 23842\n",
      "2022-09-05T01:51:02+1000 [INFO] <module> Image 20300 of 23842\n",
      "2022-09-05T01:56:37+1000 [INFO] <module> Image 20400 of 23842\n",
      "2022-09-05T02:02:12+1000 [INFO] <module> Image 20500 of 23842\n",
      "2022-09-05T02:07:47+1000 [INFO] <module> Image 20600 of 23842\n",
      "2022-09-05T02:13:22+1000 [INFO] <module> Image 20700 of 23842\n",
      "2022-09-05T02:18:58+1000 [INFO] <module> Image 20800 of 23842\n",
      "2022-09-05T02:24:33+1000 [INFO] <module> Image 20900 of 23842\n",
      "2022-09-05T02:30:08+1000 [INFO] <module> Image 21000 of 23842\n",
      "2022-09-05T02:35:43+1000 [INFO] <module> Image 21100 of 23842\n",
      "2022-09-05T02:41:19+1000 [INFO] <module> Image 21200 of 23842\n",
      "2022-09-05T02:46:55+1000 [INFO] <module> Image 21300 of 23842\n",
      "2022-09-05T02:52:31+1000 [INFO] <module> Image 21400 of 23842\n",
      "2022-09-05T02:58:06+1000 [INFO] <module> Image 21500 of 23842\n",
      "2022-09-05T03:03:41+1000 [INFO] <module> Image 21600 of 23842\n",
      "2022-09-05T03:09:18+1000 [INFO] <module> Image 21700 of 23842\n",
      "2022-09-05T03:14:54+1000 [INFO] <module> Image 21800 of 23842\n",
      "2022-09-05T03:20:30+1000 [INFO] <module> Image 21900 of 23842\n",
      "2022-09-05T03:26:06+1000 [INFO] <module> Image 22000 of 23842\n",
      "2022-09-05T03:31:42+1000 [INFO] <module> Image 22100 of 23842\n",
      "2022-09-05T03:37:17+1000 [INFO] <module> Image 22200 of 23842\n",
      "2022-09-05T03:42:52+1000 [INFO] <module> Image 22300 of 23842\n",
      "2022-09-05T03:48:28+1000 [INFO] <module> Image 22400 of 23842\n",
      "2022-09-05T03:54:04+1000 [INFO] <module> Image 22500 of 23842\n",
      "2022-09-05T03:59:40+1000 [INFO] <module> Image 22600 of 23842\n",
      "2022-09-05T04:05:16+1000 [INFO] <module> Image 22700 of 23842\n",
      "2022-09-05T04:10:52+1000 [INFO] <module> Image 22800 of 23842\n",
      "2022-09-05T04:16:27+1000 [INFO] <module> Image 22900 of 23842\n",
      "2022-09-05T04:22:04+1000 [INFO] <module> Image 23000 of 23842\n",
      "2022-09-05T04:27:40+1000 [INFO] <module> Image 23100 of 23842\n",
      "2022-09-05T04:33:17+1000 [INFO] <module> Image 23200 of 23842\n",
      "2022-09-05T04:38:53+1000 [INFO] <module> Image 23300 of 23842\n",
      "2022-09-05T04:44:30+1000 [INFO] <module> Image 23400 of 23842\n",
      "2022-09-05T04:50:06+1000 [INFO] <module> Image 23500 of 23842\n",
      "2022-09-05T04:55:42+1000 [INFO] <module> Image 23600 of 23842\n",
      "2022-09-05T05:01:18+1000 [INFO] <module> Image 23700 of 23842\n",
      "2022-09-05T05:06:54+1000 [INFO] <module> Image 23800 of 23842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "time: 22h 16min 33s (started: 2022-09-04 06:52:41 +10:00)\n"
     ]
    }
   ],
   "source": [
    "# NEW CODE\n",
    "\n",
    "# Initialization\n",
    "################\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(funcName)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%S%z\",\n",
    "    handlers=[logging.StreamHandler()])\n",
    "logging.info('Starting detect_crb_damage.ipynb')\n",
    "\n",
    "\n",
    "# Get a sorted list of image files\n",
    "\n",
    "# Intialize other variables\n",
    "\n",
    "image_files = sorted(glob.glob(IMAGE_FILE_PATH))\n",
    "num_frames = len(image_files)\n",
    "\n",
    "labels_from_csv = get_labels(CLASSES_CVAT, CLASSES_TYPE)\n",
    "final_result = {'meta':{'task': OrderedDict([('id',str(TASK_ID)),\n",
    "                                             ('name',str(TASK_NAME)),\n",
    "                                             ('size',str(num_frames)),\n",
    "                                             ('mode','interpolation'),\n",
    "                                             ('start_frame', str(0)),\n",
    "                                             ('stop_frame', str(num_frames-1)),\n",
    "                                             ('z_order',\"False\"),\n",
    "                                             ('labels', labels_from_csv)])},\n",
    "                'frames':[]}\n",
    "\n",
    "if TYPE == \"both\":\n",
    "    od_model = ObjectDetection(OD_MODEL)\n",
    "    seg_model = Segmentation(MASK_MODEL)\n",
    "elif TYPE == \"classes\":\n",
    "    od_model = ObjectDetection(OD_MODEL)\n",
    "elif TYPE == \"v_shape\":\n",
    "    seg_model = Segmentation(MASK_MODEL)\n",
    "    \n",
    "labels_mapping_od = {1:'zero',2:'light',3:'medium',4:'high',5:'non_recoverable'}\n",
    "\n",
    "# Get size of first image in list. It is assumed that all images are the same size.\n",
    "\n",
    "frame = cv2.imread(image_files[0])\n",
    "frame_height, frame_width, channels = frame.shape\n",
    "\n",
    "height, width = frame_height, frame_width\n",
    "\n",
    "# Process image files\n",
    "#####################\n",
    "\n",
    "frame_no = 0\n",
    "for image_file in image_files[0:MAX_IMAGES]:\n",
    "    frame_no += 1\n",
    "#     print(f'Image {frame_no} of {num_frames}')\n",
    "    frame = cv2.imread(image_file)\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_np_expanded = np.expand_dims(img, axis=0)\n",
    "\n",
    "    od_result = {}\n",
    "    result = {}\n",
    "    if TYPE == \"both\" or TYPE == \"classes\":\n",
    "        # run detection\n",
    "        boxes, scores, classes, num_detections = od_model.get_detections(image_np_expanded)\n",
    "        #normalize bounding boxes, also apply threshold\n",
    "        od_result = ObjectDetection.process_boxes(boxes, scores, classes, labels_mapping_od, OD_THRESHOLD, width, height)\n",
    "        if od_result:\n",
    "            #print(\"od\", od_result)\n",
    "            shapes = []\n",
    "            for label, boxes in od_result.items():\n",
    "                for box in boxes:\n",
    "                    shapes.append({'type':'rectangle','label':label,'occluded':0,'points':box})\n",
    "            final_result['frames'].append({'frame':frame_no, 'width':frame_width, 'height':frame_height, 'shapes':shapes})\n",
    "    if TYPE == \"both\" or TYPE == \"v_shape\":\n",
    "        # run segmentation\n",
    "        result = seg_model.get_polygons([img], MASK_THRESHOLD)\n",
    "        #print(\"Result before processing: \", result)\n",
    "        if TYPE == \"both\" or TYPE == \"classes\":\n",
    "            # filter out false positives if boxes are available\n",
    "            result = Segmentation.process_polygons(result, od_result)\n",
    "            #print(\"Result after processing: \", result)\n",
    "        if result:\n",
    "            shapes = []\n",
    "            for label, polygons in result.items():\n",
    "                for polygon in polygons:\n",
    "                    shapes.append({'type':'polygon','label':label,'occluded':0,'points':polygon})\n",
    "            frame_exists = False\n",
    "            for frame_ in final_result['frames']:\n",
    "                if frame_['frame'] == frame_no:\n",
    "                    break\n",
    "            if frame_exists:\n",
    "                final_result['frames']['shapes'].extend(shapes)\n",
    "            else:\n",
    "                final_result['frames'].append({'frame':frame_no, 'width':frame_width, 'height':frame_height, 'shapes':shapes})\n",
    "        if (frame_no % 100 == 0):\n",
    "            logging.info(f'Image {frame_no} of {num_frames}')\n",
    "                        \n",
    "#frame = draw_instances(frame, od_result, result)\n",
    "dump_as_cvat_annotation(open(OUTPUT_XML_PATH, \"w\"), final_result)\n",
    "\n",
    "print('FINISHED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
